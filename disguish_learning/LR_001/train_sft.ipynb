{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5113edbc-e1b9-4739-90c8-124b11244e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/generativeModel/disguish_learning/LR_001\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74580f47-898f-4be5-ad3d-92f08e007b17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e147e63e-f514-4f24-a9c6-c16a954b7c40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir('/root/autodl-tmp/generativeModel/disguish_learning/LR_001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc0f2c1f-5e0d-4fea-b3c5-900f22e04cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import  AutoModelForSeq2SeqLM,AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e190d5d-f93b-4f7d-b3e3-90f5fab882e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from peft import (\n",
    "    prepare_model_for_int8_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    ")\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fa117c7-3344-414a-95c9-2a87dc7372e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Some weights of the model checkpoint at THUDM/glm-2b were not used when initializing GLMForConditionalGeneration: ['out_proj.weight', 'out_proj.bias', 'dense.weight', 'dense.bias']\n",
      "- This IS expected if you are initializing GLMForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GLMForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "cache_dir = '/root/autodl-tmp/model/'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('THUDM/glm-2b',cache_dir=cache_dir,\n",
    "                                              trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ee29ed8-5d9b-4211-a53d-cf405d66952e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/peft/tuners/lora.py:180: UserWarning: fan_in_fan_out is set to True but the target module is not a Conv1D. Setting fan_in_fan_out to False.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_int8_training(model)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,#LORA_R,\n",
    "    lora_alpha=16,#LORA_ALPHA,\n",
    "    target_modules=[\"query_key_value\"],#TARGET_MODULES,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model,lora_config)#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48935299-5ab6-446d-b849-19dd1c00ab6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_cache_dir = '/root/autodl-tmp/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a74f9efb-8763-48c1-bc99-7bf0698374c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration cahya--instructions-zh-507237297bfcf9f5\n",
      "Found cached dataset parquet (/root/autodl-tmp/data/cahya___parquet/cahya--instructions-zh-507237297bfcf9f5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010710954666137695,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612978cdcdcc46f9b8db3fbb1e1579f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"cahya/instructions-zh\",cache_dir=data_cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4b85007-c9e0-4498-9e59-626c65373bbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 42216, 'text': 'User: 虚假信息与虚假信息有什么区别?\\nAssistant: 误导和误导之间有区别 - 误导是欺骗的使用来破坏沟通过程,而误导是错误的信息或事实的误解。 误导的一个常见例子是新闻来源故意报告虚假声明或误导性信息以传播宣传和影响意见。'}\n"
     ]
    }
   ],
   "source": [
    "for x in data['train']:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5899948-327b-4e84-8bc5-323d0963296f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('THUDM/glm-2b',cache_dir=cache_dir,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "080e28ae-4a17-4293-bebe-bef8b68a5476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_train_example(text:str):\n",
    "    answer_prefix = \"Assistant:\"\n",
    "    prompt_prefix = \"User:\"\n",
    "\n",
    "    answer_start_idx = text.find(answer_prefix)\n",
    "    if(answer_start_idx > 0):\n",
    "        # this is an trian data\n",
    "        answer = text[answer_start_idx + len(answer_prefix):]\n",
    "        prompt = text[:answer_start_idx].replace(prompt_prefix,\"\")\n",
    "    else:\n",
    "        prompt = text\n",
    "        answer = None\n",
    "\n",
    "    return prompt,answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12a93d10-a6f6-469d-af48-4dc303c2470b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_tokenzie_func(tokenizer,pad_idx=0,max_length=256,pad=True):\n",
    "    def tokenize(example):\n",
    "        text = example['text']\n",
    "        prompt,answer = split_train_example(text)\n",
    "        prompt_idxs = tokenizer(prompt)\n",
    "        answer_idxs = tokenizer(answer) if answer is not None else []\n",
    "\n",
    "        example = {}\n",
    "        example['input_ids'] = prompt_idxs['input_ids'] + answer_idxs['input_ids']\n",
    "        example['attention_mask'] = prompt_idxs['attention_mask'] + answer_idxs['attention_mask']\n",
    "        example['labels'] = [0] * len(prompt_idxs['attention_mask']) + answer_idxs['input_ids']\n",
    "        example['prompt'] = prompt\n",
    "        example['answer'] = answer\n",
    "        \n",
    "        if(len(example['input_ids']) < max_length):\n",
    "            count = max_length - len(example['input_ids'])\n",
    "            example['input_ids'] = example['input_ids'] + [0] * count\n",
    "            example['attention_mask'] = example['attention_mask']+ [0] * count\n",
    "            example['labels'] = example['labels']+ [0] * count\n",
    "        \n",
    "        example['input_ids'] = example['input_ids'][:max_length]\n",
    "        example['attention_mask'] = example['attention_mask'][:max_length]\n",
    "        example['labels'] = example['labels'][:max_length]\n",
    "        \n",
    "\n",
    "        return example\n",
    "    return tokenize\n",
    "tokenize_func = build_tokenzie_func(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfcdf15c-833a-4a0e-adad-a94031cac6f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/autodl-tmp/data/cahya___parquet/cahya--instructions-zh-507237297bfcf9f5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-8aaf354e31de97d3.arrow\n"
     ]
    }
   ],
   "source": [
    "train_data = data['train'].map(tokenize_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caeb3d2b-50c6-41b5-ba5b-518ba76f0b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 42216, 'text': 'User: 虚假信息与虚假信息有什么区别?\\nAssistant: 误导和误导之间有区别 - 误导是欺骗的使用来破坏沟通过程,而误导是错误的信息或事实的误解。 误导的一个常见例子是新闻来源故意报告虚假声明或误导性信息以传播宣传和影响意见。', 'input_ids': [50259, 5525, 247, 248, 161, 223, 229, 46479, 94, 162, 223, 107, 10310, 236, 164, 247, 248, 161, 223, 229, 46479, 94, 162, 223, 107, 17312, 231, 20015, 222, 20046, 230, 44293, 118, 26344, 104, 30, 198, 50256, 50259, 5525, 107, 107, 43380, 120, 161, 240, 234, 46237, 107, 43380, 120, 45298, 29785, 112, 17312, 231, 44293, 118, 26344, 104, 532, 5525, 107, 107, 43380, 120, 42468, 162, 105, 118, 165, 103, 245, 21410, 45635, 18796, 101, 30266, 98, 163, 254, 112, 161, 251, 237, 162, 110, 253, 34460, 248, 32573, 229, 163, 101, 233, 11, 32003, 234, 46237, 107, 43380, 120, 42468, 165, 242, 247, 46237, 107, 21410, 46479, 94, 162, 223, 107, 22755, 244, 12859, 233, 22522, 252, 21410, 46237, 107, 164, 100, 96, 16764, 5525, 107, 107, 43380, 120, 21410, 31660, 10310, 103, 30585, 116, 164, 100, 223, 160, 122, 233, 36310, 42468, 23877, 108, 29785, 119, 30266, 98, 162, 118, 238, 46763, 227, 35707, 237, 162, 232, 98, 37772, 232, 164, 247, 248, 161, 223, 229, 18004, 108, 23626, 236, 22755, 244, 46237, 107, 43380, 120, 45250, 100, 46479, 94, 162, 223, 107, 20015, 98, 27670, 254, 162, 240, 255, 22522, 96, 27670, 254, 161, 240, 234, 37605, 109, 161, 241, 235, 35707, 237, 164, 100, 223, 16764, 50256, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50259, 5525, 107, 107, 43380, 120, 161, 240, 234, 46237, 107, 43380, 120, 45298, 29785, 112, 17312, 231, 44293, 118, 26344, 104, 532, 5525, 107, 107, 43380, 120, 42468, 162, 105, 118, 165, 103, 245, 21410, 45635, 18796, 101, 30266, 98, 163, 254, 112, 161, 251, 237, 162, 110, 253, 34460, 248, 32573, 229, 163, 101, 233, 11, 32003, 234, 46237, 107, 43380, 120, 42468, 165, 242, 247, 46237, 107, 21410, 46479, 94, 162, 223, 107, 22755, 244, 12859, 233, 22522, 252, 21410, 46237, 107, 164, 100, 96, 16764, 5525, 107, 107, 43380, 120, 21410, 31660, 10310, 103, 30585, 116, 164, 100, 223, 160, 122, 233, 36310, 42468, 23877, 108, 29785, 119, 30266, 98, 162, 118, 238, 46763, 227, 35707, 237, 162, 232, 98, 37772, 232, 164, 247, 248, 161, 223, 229, 18004, 108, 23626, 236, 22755, 244, 46237, 107, 43380, 120, 45250, 100, 46479, 94, 162, 223, 107, 20015, 98, 27670, 254, 162, 240, 255, 22522, 96, 27670, 254, 161, 240, 234, 37605, 109, 161, 241, 235, 35707, 237, 164, 100, 223, 16764, 50256, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'prompt': ' 虚假信息与虚假信息有什么区别?\\n', 'answer': ' 误导和误导之间有区别 - 误导是欺骗的使用来破坏沟通过程,而误导是错误的信息或事实的误解。 误导的一个常见例子是新闻来源故意报告虚假声明或误导性信息以传播宣传和影响意见。'}\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12ad189f-3b45-4149-9ceb-288212bb708c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009716987609863281,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2023,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7ee1d24ef147e7847cb0c60a75d913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2023 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008729219436645508,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2024,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabde4ebaa6f40368930415b0e3ffb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2024 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_data = data['validation'].map(tokenize_func)\n",
    "test_data = data['test'].map(tokenize_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b2a5b22-ce17-4118-97dd-862be0f32f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddp = True if torch.cuda.device_count() > 1 else False\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=128 // 4,\n",
    "        warmup_steps=100,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=1e-5,\n",
    "        fp16=True,\n",
    "        logging_steps=20,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        save_steps=200,\n",
    "        output_dir=\"lora-alpaca\",\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        ddp_find_unused_parameters=False if ddp else None,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "old_state_dict = model.state_dict\n",
    "model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n",
    ").__get__(model, type(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ad793-dd06-44e9-ab3b-e1c7fd6b7348",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='442' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [442/600 1:02:59 < 22:37, 0.12 it/s, Epoch 0.73/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>7.376900</td>\n",
       "      <td>7.324034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.904300</td>\n",
       "      <td>4.826227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if torch.__version__ >= \"2\":\n",
    "    model = torch.compile(model)\n",
    "\n",
    "##############\n",
    "# 训练循环\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "model.save_pretrained(\"lora-alpaca\")\n",
    "\n",
    "print(\"\\n If there's a warning about missing keys above, please disregard :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a445cabb-5ab4-42b6-8504-4fb04cdc728d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Assistant:你好，我的第一个›助理\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e586b218-61e3-4680-8a2a-418395dad607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_idxs = tokenizer(prompt)\n",
    "prompt_idxs = {\n",
    "    key : torch.LongTensor(prompt_idxs[key])\n",
    "    for key in prompt_idxs\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a001cc-1f0d-4b86-bfff-8083e51c6018",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2bf03-943d-4108-9390-d5122b67063d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch._dynamo.config.suppress_errors = True\n",
    "model(input_ids=prompt_idxs['input_ids'],attention_mask=prompt_idxs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f20b1-2891-4001-ac12-43c91a5102bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
