## 尝试一些RL的算法在语言模型微调伤的应用
*** 
- 目前在RLHF这一部分普遍认为就是为了学会如何区分：
  - 合适的和不合适的
     * 有意义和无意义
       * 有关的和无关的
       * 重复的和新颖的
     * 有害的和无害的
       * 攻击性和温和的
       * 
- 现在的方式
  * 用reward model的方式
    * reward model提供了一定的泛化性，允许语言模型进行探索和利用（优化）
    * 允许模型以一定的方式进行sample自己的答案。然后，综合的按照上面的标准进行排序
    * 但reward model预估是不是会有误差传播？
    * 或者其实reward model的方式反倒更加ok，因为提供《提供了一定的泛化性，允许语言模型进行探索和利用》
    * 他到底学到了哪些东西呢？
  * 用rank-based的方式
    * 采用排序的模型，直接对比上面的标准
    * 构造会相对简单一些，不需要一个reward model的构造
    * 但是相对而言，OOD上或者泛化上效果一般
* 所以我现在想
  * 是否可以用更新的RL的办法
    * 不直接建模reward model（世界模型），而且采用其他办法呢？比如AC框架？
      * 但好像没法直接进行世界采样
    * Batch-RL，采用离线强化学习的方式。
