{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5113edbc-e1b9-4739-90c8-124b11244e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/generativeModel/disguish_learning/LR_001\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74580f47-898f-4be5-ad3d-92f08e007b17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e147e63e-f514-4f24-a9c6-c16a954b7c40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir('/root/autodl-tmp/generativeModel/disguish_learning/LR_001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc0f2c1f-5e0d-4fea-b3c5-900f22e04cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import  AutoModelForCausalLM,AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e190d5d-f93b-4f7d-b3e3-90f5fab882e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from peft import (\n",
    "    prepare_model_for_int8_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    ")\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df430052-c69d-433d-aee1-58c81264f3c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fa117c7-3344-414a-95c9-2a87dc7372e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bigscience/bloom-3b/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f078c98c130>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/bigscience/bloom-3b/resolve/main/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "cache_dir = '/root/autodl-tmp/model/'\n",
    "model = AutoModelForCausalLM.from_pretrained('bigscience/bloom-3b',cache_dir=cache_dir,trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bigscience/bloom-3b',cache_dir=cache_dir,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e265c5ef-00dc-4da4-ab56-1b208e0666f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67c3a5d6-a8c6-4328-a76a-5999bf9f5fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make model as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ee29ed8-5d9b-4211-a53d-cf405d66952e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/peft/tuners/lora.py:180: UserWarning: fan_in_fan_out is set to True but the target module is not a Conv1D. Setting fan_in_fan_out to False.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_int8_training(model)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,#LORA_R,\n",
    "    lora_alpha=16,#LORA_ALPHA,\n",
    "    target_modules=[\"query_key_value\"],#TARGET_MODULES,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model,lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48935299-5ab6-446d-b849-19dd1c00ab6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_cache_dir = '/root/autodl-tmp/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a74f9efb-8763-48c1-bc99-7bf0698374c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration cahya--instructions-zh-507237297bfcf9f5\n",
      "Found cached dataset parquet (/root/autodl-tmp/data/cahya___parquet/cahya--instructions-zh-507237297bfcf9f5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011494874954223633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa1d6ad6c4a406ab38174f89264f6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"cahya/instructions-zh\",cache_dir=data_cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4b85007-c9e0-4498-9e59-626c65373bbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 42216, 'text': 'User: 虚假信息与虚假信息有什么区别?\\nAssistant: 误导和误导之间有区别 - 误导是欺骗的使用来破坏沟通过程,而误导是错误的信息或事实的误解。 误导的一个常见例子是新闻来源故意报告虚假声明或误导性信息以传播宣传和影响意见。'}\n"
     ]
    }
   ],
   "source": [
    "for x in data['train']:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5899948-327b-4e84-8bc5-323d0963296f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "080e28ae-4a17-4293-bebe-bef8b68a5476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_train_example(text:str):\n",
    "    answer_prefix = \"Assistant:\"\n",
    "    prompt_prefix = \"User:\"\n",
    "\n",
    "    answer_start_idx = text.rfind(answer_prefix)\n",
    "    if(answer_start_idx > 0):\n",
    "        # this is an trian data\n",
    "        answer = text[answer_start_idx + len(answer_prefix):]\n",
    "        prompt = text[:(answer_start_idx + len(answer_prefix):)]#.replace(prompt_prefix,\"\")\n",
    "    else:\n",
    "        prompt = text\n",
    "        answer = None\n",
    "\n",
    "    return prompt,answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12a93d10-a6f6-469d-af48-4dc303c2470b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_tokenzie_func(tokenizer,pad_idx=0,max_length=256,pad=True):\n",
    "    def tokenize(example):\n",
    "        text = example['text']\n",
    "        prompt,answer = split_train_example(text)\n",
    "        prompt_idxs = tokenizer(prompt)\n",
    "        answer_idxs = tokenizer(answer) if answer is not None else []\n",
    "\n",
    "        example = {}\n",
    "        example['input_ids'] = prompt_idxs['input_ids'] + answer_idxs['input_ids']\n",
    "        example['attention_mask'] = prompt_idxs['attention_mask'] + answer_idxs['attention_mask']\n",
    "        example['labels'] = [0] * len(prompt_idxs['attention_mask']) + answer_idxs['input_ids']\n",
    "        example['prompt'] = prompt\n",
    "        example['answer'] = answer\n",
    "        \n",
    "        if(len(example['input_ids']) < max_length):\n",
    "            count = max_length - len(example['input_ids'])\n",
    "            example['input_ids'] = example['input_ids'] + [0] * count\n",
    "            example['attention_mask'] = example['attention_mask']+ [0] * count\n",
    "            example['labels'] = example['labels']+ [0] * count\n",
    "        \n",
    "        example['input_ids'] = example['input_ids'][:max_length]\n",
    "        example['attention_mask'] = example['attention_mask'][:max_length]\n",
    "        example['labels'] = example['labels'][:max_length]\n",
    "        \n",
    "\n",
    "        return example\n",
    "    return tokenize\n",
    "tokenize_func = build_tokenzie_func(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfcdf15c-833a-4a0e-adad-a94031cac6f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/autodl-tmp/data/cahya___parquet/cahya--instructions-zh-507237297bfcf9f5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b67f8effbb24490f.arrow\n"
     ]
    }
   ],
   "source": [
    "train_data = data['train'].map(tokenize_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caeb3d2b-50c6-41b5-ba5b-518ba76f0b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 42216, 'text': 'User: 虚假信息与虚假信息有什么区别?\\nAssistant: 误导和误导之间有区别 - 误导是欺骗的使用来破坏沟通过程,而误导是错误的信息或事实的误解。 误导的一个常见例子是新闻来源故意报告虚假声明或误导性信息以传播宣传和影响意见。', 'input_ids': [210, 238785, 8401, 1961, 238785, 8401, 37242, 49076, 1021, 210, 19651, 5092, 745, 19651, 5092, 10040, 706, 49076, 647, 210, 19651, 5092, 644, 151711, 60697, 1572, 30949, 28543, 5425, 2818, 15, 1518, 19651, 5092, 644, 35892, 42287, 2140, 28866, 373, 201933, 420, 210, 19651, 5092, 16368, 39428, 40567, 644, 19312, 28349, 70251, 4840, 238785, 32460, 2140, 19651, 5092, 1963, 8401, 909, 35786, 26262, 745, 7962, 15344, 420, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0, 0, 0, 0, 0, 0, 0, 0, 0, 210, 19651, 5092, 745, 19651, 5092, 10040, 706, 49076, 647, 210, 19651, 5092, 644, 151711, 60697, 1572, 30949, 28543, 5425, 2818, 15, 1518, 19651, 5092, 644, 35892, 42287, 2140, 28866, 373, 201933, 420, 210, 19651, 5092, 16368, 39428, 40567, 644, 19312, 28349, 70251, 4840, 238785, 32460, 2140, 19651, 5092, 1963, 8401, 909, 35786, 26262, 745, 7962, 15344, 420, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'prompt': ' 虚假信息与虚假信息有什么区别?\\n', 'answer': ' 误导和误导之间有区别 - 误导是欺骗的使用来破坏沟通过程,而误导是错误的信息或事实的误解。 误导的一个常见例子是新闻来源故意报告虚假声明或误导性信息以传播宣传和影响意见。'}\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12ad189f-3b45-4149-9ceb-288212bb708c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/autodl-tmp/data/cahya___parquet/cahya--instructions-zh-507237297bfcf9f5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b436db858b02878b.arrow\n",
      "Loading cached processed dataset at /root/autodl-tmp/data/cahya___parquet/cahya--instructions-zh-507237297bfcf9f5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-ff0a77ded71f3fb7.arrow\n"
     ]
    }
   ],
   "source": [
    "val_data = data['validation'].map(tokenize_func)\n",
    "test_data = data['test'].map(tokenize_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b2a5b22-ce17-4118-97dd-862be0f32f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddp = True if torch.cuda.device_count() > 1 else False\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=128//2,\n",
    "        warmup_steps=100,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=1e-5,\n",
    "        fp16=True,\n",
    "        logging_steps=2000,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=2000,\n",
    "        save_steps=2000,\n",
    "        output_dir=\"lora-alpaca\",\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        ddp_find_unused_parameters=False if ddp else None,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "old_state_dict = model.state_dict\n",
    "model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n",
    ").__get__(model, type(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac2ad793-dd06-44e9-ab3b-e1c7fd6b7348",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31709' max='57666' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31709/57666 3:33:10 < 2:54:31, 2.48 it/s, Epoch 1.65/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.689000</td>\n",
       "      <td>0.728290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.695600</td>\n",
       "      <td>0.692487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.676100</td>\n",
       "      <td>0.679073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.662000</td>\n",
       "      <td>0.670674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.665600</td>\n",
       "      <td>0.664975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.657200</td>\n",
       "      <td>0.661186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.657600</td>\n",
       "      <td>0.657844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.652300</td>\n",
       "      <td>0.655650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.653018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.650600</td>\n",
       "      <td>0.651126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.638100</td>\n",
       "      <td>0.649634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.644300</td>\n",
       "      <td>0.648253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.644600</td>\n",
       "      <td>0.647051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.639600</td>\n",
       "      <td>0.646278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.636700</td>\n",
       "      <td>0.645322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m     model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcompile(model)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m##############\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 训练循环\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlora-alpaca\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m If there\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms a warning about missing keys above, please disregard :)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/trainer.py:1639\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1634\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1636\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1638\u001b[0m )\n\u001b[0;32m-> 1639\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1640\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1644\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/trainer.py:1906\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1904\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1905\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1906\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1909\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1910\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1911\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1912\u001b[0m ):\n\u001b[1;32m   1913\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1914\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/trainer.py:2662\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[1;32m   2661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_grad_scaling:\n\u001b[0;32m-> 2662\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2663\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_apex:\n\u001b[1;32m   2664\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mscale_loss(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer) \u001b[38;5;28;01mas\u001b[39;00m scaled_loss:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if torch.__version__ >= \"2\":\n",
    "    model = torch.compile(model)\n",
    "\n",
    "##############\n",
    "# 训练循环\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n If there's a warning about missing keys above, please disregard :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da20101-f839-45e9-930c-d88c2b19c8de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d72512-a7ce-42ef-a66d-4c96a936fa3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4080940d-5a1c-43cf-94db-0576a01c7ee1",
   "metadata": {},
   "source": [
    "# 测试部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a445cabb-5ab4-42b6-8504-4fb04cdc728d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e586b218-61e3-4680-8a2a-418395dad607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd6d1262-912b-4578-9559-a3cfaf566cd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt_idxs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprompt_idxs\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt_idxs' is not defined"
     ]
    }
   ],
   "source": [
    "prompt_idxs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc454163-e53a-4bef-a55f-c8d27ac2cb8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt_idxs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprompt_idxs\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt_idxs' is not defined"
     ]
    }
   ],
   "source": [
    "prompt_idxs['input_ids'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a001cc-1f0d-4b86-bfff-8083e51c6018",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.disable_adapter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f3db065-4618-40ac-ae28-9c0c8bb3f32a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250880, 2560])\n",
      "torch.Size([2560])\n",
      "torch.Size([2560])\n"
     ]
    }
   ],
   "source": [
    "for n,x in model.base_model.model.named_parameters():\n",
    "    if(\"embedding\" in n):\n",
    "        print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2169cd33-8aa1-4a3f-8e68-361714246183",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "User:习近平是谁么\n",
    "Assistant:\n",
    "习近平是中国共产党总书记,也是中国国家主席。\n",
    "User:他有什么职责\n",
    "Assistant:习近平负责领导中国共产党和中国政府,并监督中国的政治和经济。\n",
    "User:我们在聊谁呢？\n",
    "\"\"\"\n",
    "prompt_idxs = tokenizer(prompt)\n",
    "\n",
    "prompt_idxs = {\n",
    "    key : torch.LongTensor([prompt_idxs[key]]).cuda()\n",
    "    for key in prompt_idxs\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8be2bf03-943d-4108-9390-d5122b67063d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "   x = model.generate(input_ids=prompt_idxs['input_ids'],max_new_tokens=56)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af3f20b1-2891-4001-ac12-43c91a5102bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User:习近平是谁么\n",
      "Assistant:\n",
      "习近平是中国共产党总书记,也是中国国家主席。\n",
      "User:他有什么职责\n",
      "Assistant:习近平负责领导中国共产党和中国政府,并监督中国的政治和经济。\n",
      "User:我们在聊谁呢？\n",
      "Assistant:习近平是中国共产党总书记,也是中国国家主席。<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n"
     ]
    }
   ],
   "source": [
    "for z in x.cpu().numpy():   \n",
    "    y= tokenizer.decode(z)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94897c6c-efbc-4856-ab89-f379e9c7ce3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10f50f37-2189-456f-b13f-c94ca947911b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora-alpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc7f86-b68a-4d99-b1f2-15d4f8c9d40d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
